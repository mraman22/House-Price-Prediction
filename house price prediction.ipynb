{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2846f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-02T07:48:00.812933Z",
     "iopub.status.busy": "2021-10-02T07:48:00.804458Z",
     "iopub.status.idle": "2021-10-02T07:48:06.965624Z",
     "shell.execute_reply": "2021-10-02T07:48:06.964901Z",
     "shell.execute_reply.started": "2021-10-02T07:45:05.756474Z"
    },
    "papermill": {
     "duration": 6.175843,
     "end_time": "2021-10-02T07:48:06.965803",
     "exception": false,
     "start_time": "2021-10-02T07:48:00.789960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-02 07:48:02.705260: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n",
      "2021-10-02 07:48:02.705386: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.ensemble\n",
    "import sklearn.kernel_ridge\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import sklearn.svm\n",
    "import tensorflow as tf\n",
    "pd.set_option('display.max_columns', 100)\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff68d5f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-02T07:48:06.986620Z",
     "iopub.status.busy": "2021-10-02T07:48:06.985646Z",
     "iopub.status.idle": "2021-10-02T07:48:07.073625Z",
     "shell.execute_reply": "2021-10-02T07:48:07.073064Z",
     "shell.execute_reply.started": "2021-10-02T07:45:05.764768Z"
    },
    "papermill": {
     "duration": 0.099403,
     "end_time": "2021-10-02T07:48:07.073779",
     "exception": false,
     "start_time": "2021-10-02T07:48:06.974376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read CSVs\n",
    "dirname = '/kaggle/input/house-prices-advanced-regression-techniques/'\n",
    "benchmark = pd.read_csv(dirname + 'sample_submission.csv')\n",
    "train = pd.read_csv(dirname + 'train.csv')\n",
    "test = pd.read_csv(dirname + 'test.csv')\n",
    "# train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9ec922e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-02T07:48:07.097663Z",
     "iopub.status.busy": "2021-10-02T07:48:07.097030Z",
     "iopub.status.idle": "2021-10-02T07:48:07.254955Z",
     "shell.execute_reply": "2021-10-02T07:48:07.256438Z",
     "shell.execute_reply.started": "2021-10-02T07:45:05.811365Z"
    },
    "papermill": {
     "duration": 0.174994,
     "end_time": "2021-10-02T07:48:07.256900",
     "exception": false,
     "start_time": "2021-10-02T07:48:07.081906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark score:  0.7915365120379597\n"
     ]
    }
   ],
   "source": [
    "# Benchmark: random integer mapping, regression on features with |corr| > 0.3\n",
    "cols = []\n",
    "benchmark_train = train.copy().drop([\"Id\", \"SalePrice\"], axis=1)\n",
    "\n",
    "for col in benchmark_train.columns:\n",
    "    \n",
    "    na_cnt = benchmark_train[col].isna().sum()\n",
    "    \n",
    "    if benchmark_train[col].dtype != int and benchmark_train[col].dtype != float:\n",
    "        mappings = zip(benchmark_train[col].unique(), range(len(benchmark_train[col].unique())))\n",
    "        benchmark_train[col] = benchmark_train[col].map(dict(mappings))\n",
    "    \n",
    "#     print('{}: {} null & corr {}'.format(\n",
    "#         col, \n",
    "#         na_cnt, \n",
    "#         benchmark_train[col].corr(train['SalePrice'])\n",
    "#     ))  \n",
    "    \n",
    "    if abs(benchmark_train[col].corr(train['SalePrice'])) > 0.3 and benchmark_train[col].isna().sum() == 0:\n",
    "        cols.append(col)\n",
    "    \n",
    "lr = sklearn.linear_model.LinearRegression()\n",
    "lr.fit(benchmark_train[cols], train['SalePrice'])\n",
    "# benchmark_prediction = lr.predict(test[cols])\n",
    "print('Benchmark score: ', lr.score(benchmark_train[cols], train['SalePrice']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87550907",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-02T07:48:07.302943Z",
     "iopub.status.busy": "2021-10-02T07:48:07.301951Z",
     "iopub.status.idle": "2021-10-02T07:48:07.327529Z",
     "shell.execute_reply": "2021-10-02T07:48:07.328313Z",
     "shell.execute_reply.started": "2021-10-02T07:45:05.954729Z"
    },
    "papermill": {
     "duration": 0.055646,
     "end_time": "2021-10-02T07:48:07.328592",
     "exception": false,
     "start_time": "2021-10-02T07:48:07.272946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions for data preprocessing\n",
    "stats_format_categorical = '''\n",
    "~ Summary of {} ~\n",
    "\n",
    "# of null values : {}\n",
    "# of unique values : {}\n",
    "# of occurrences of most frequent value: {}\n",
    "Correlation ~ price : {}\n",
    "__________________________________\n",
    "Value Counts\n",
    "{}\n",
    "__________________________________\n",
    "Mean Price\n",
    "{}\n",
    "\n",
    "'''\n",
    "\n",
    "stats_format_continuous = '''\n",
    "~ Summary of {} ~\n",
    "\n",
    "# of 0s : {}\n",
    "# of null values : {}\n",
    "Correlation ~ price : {}\n",
    "__________________________________\n",
    "\n",
    "'''\n",
    "\n",
    "def check_categorical(train, colname, map_numerical=False):\n",
    "    if map_numerical:\n",
    "        mapping = dict(zip(train[colname].unique(), range(train[colname].unique().size)))\n",
    "    print(stats_format_categorical.format(\n",
    "        colname, \n",
    "        train[colname].isna().sum(), \n",
    "        train[colname].unique().size,\n",
    "        (train[colname] == train[colname].mode()[0]).sum(),\n",
    "        train[colname].map(mapping).corr(train['SalePrice']) if map_numerical else train[colname].corr(train['SalePrice']),\n",
    "        train[colname].value_counts(dropna=False),\n",
    "        train[[colname, 'SalePrice']].groupby(colname, as_index=False, dropna=False).mean().sort_values(by='SalePrice')\n",
    "    ))\n",
    "\n",
    "def generate_mapping(train, colname):\n",
    "    return dict(zip(train[[colname, 'SalePrice']].groupby(\n",
    "        colname, as_index=False, dropna=False\n",
    "        ).mean().sort_values(by='SalePrice')[colname], range(\n",
    "        train[colname].unique().size)))\n",
    "\n",
    "def process_categorical(train, colname, test=None, show_stats=True):\n",
    "    if show_stats:\n",
    "        print('Before:')\n",
    "        check_categorical(train, colname, map_numerical=True)\n",
    "    \n",
    "    mapping = generate_mapping(train, colname)\n",
    "    train[colname] = train[colname].map(mapping).astype(int)\n",
    "    \n",
    "    if test is not None:\n",
    "        test[colname] = test[colname].map(mapping)\n",
    "        test[colname].fillna(test[colname].mode().values[0], inplace=True)\n",
    "        test[colname] = test[colname].astype(int)\n",
    "    \n",
    "    if show_stats:\n",
    "        print('Mapping: {}\\n'.format(mapping))\n",
    "        print('After:')\n",
    "        check_categorical(train, colname)\n",
    "        sns.regplot(x=train[colname], y=train['SalePrice'])\n",
    "\n",
    "def band_categorical(train, colname, test=None, buckets=8):\n",
    "    bandname = colname + \"Band\"\n",
    "\n",
    "    _, bins = pd.qcut(train[colname], buckets, retbins=True)\n",
    "    train.insert(loc=len(train.columns), column=bandname, value=-1)\n",
    "    if test is not None:\n",
    "        test.insert(loc=len(test.columns), column=bandname, value=-1)\n",
    "    bins[0], bins[-1] = float(\"-inf\"), float(\"inf\")\n",
    "\n",
    "    for i in range(buckets):\n",
    "        train.loc[(train[colname] >= bins[i]) \n",
    "                & (train[colname] < bins[i + 1]), bandname] = i\n",
    "        if test is not None:\n",
    "            test.loc[(test[colname] >= bins[i]) \n",
    "                   & (test[colname] < bins[i + 1]), bandname] = i\n",
    "    \n",
    "def check_continuous(train, colname):\n",
    "    print(stats_format_continuous.format(\n",
    "        colname, \n",
    "        (train[colname] == 0).sum(),\n",
    "        train[colname].isna().sum(), \n",
    "        train[colname].corr(train['SalePrice']),\n",
    "    ))\n",
    "    sns.regplot(x=train[colname], y=train['SalePrice'])\n",
    "\n",
    "def process_continuous(train, colname, test=None, show_stats=True):\n",
    "    minval = train[colname].min()\n",
    "    maxval = train[colname].max()\n",
    "    train[colname] = (train[colname] - minval) / (maxval - minval)\n",
    "    if test is not None:\n",
    "        test[colname] = (test[colname] - minval) / (maxval - minval)\n",
    "    if show_stats:\n",
    "        check_continuous(train, colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfacd49b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-02T07:48:07.372487Z",
     "iopub.status.busy": "2021-10-02T07:48:07.371832Z",
     "iopub.status.idle": "2021-10-02T07:48:07.374855Z",
     "shell.execute_reply": "2021-10-02T07:48:07.374260Z",
     "shell.execute_reply.started": "2021-10-02T07:45:05.990573Z"
    },
    "papermill": {
     "duration": 0.030938,
     "end_time": "2021-10-02T07:48:07.374989",
     "exception": false,
     "start_time": "2021-10-02T07:48:07.344051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define and group feature columns\n",
    "cols_to_drop = [# Data extremely skewed\n",
    "                'Street', 'Alley', 'Utilities', 'Condition2', 'LandSlope',\n",
    "                'BsmtFinSF2', 'RoofMatl', 'BsmtFullBath', 'GarageCond',\n",
    "                'Heating', 'Electrical', 'LowQualFinSF', 'EnclosedPorch', \n",
    "                'BsmtHalfBath', 'KitchenAbvGr', 'Functional', 'PavedDrive',\n",
    "                '3SsnPorch', 'ScreenPorch',  'PoolArea', 'GarageQual', \n",
    "                'PoolQC', 'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold',\n",
    "                'LandContour', 'Condition1', 'BldgType', 'LotConfig', 'RoofStyle',\n",
    "                'ExterCond', 'BsmtCond', 'BsmtFinType2', 'SaleCondition',\n",
    "                'BedroomAbvGr', 'CentralAir', 'SaleType',\n",
    "                'Id',          # Unrelated to price\n",
    "                'LotFrontage', # Too many null values\n",
    "                'GarageYrBlt', # YearBuilt has better correlation\n",
    "                'Exterior2nd', # High correlation with Exterior1st\n",
    "                'TotalBsmtSF', # High correlation with 1stFlrSF\n",
    "                '2ndFlrSF',    # Some correlation with 1stFlrSF and too many 0s\n",
    "               ]\n",
    "cols_categorical = ['MSSubClass', 'MSZoning', 'LotShape', 'HouseStyle', \n",
    "                    'OverallQual', 'OverallCond', 'Neighborhood', \n",
    "                    'ExterQual', 'Foundation', 'Exterior1st',\n",
    "                    'BsmtQual', 'BsmtExposure', 'MasVnrType', \n",
    "                    'BsmtFinType1', 'HeatingQC', 'KitchenQual', 'TotRmsAbvGrd', \n",
    "                    'Fireplaces', 'FireplaceQu', 'GarageType', 'FullBath', \n",
    "                    'GarageFinish', 'GarageCars', 'HalfBath']\n",
    "cols_continuous = ['LotArea', '1stFlrSF', 'GrLivArea','GarageArea', 'OpenPorchSF',\n",
    "                   'BsmtFinSF1', 'BsmtUnfSF', 'MasVnrArea', 'WoodDeckSF']\n",
    "cols_to_band = ['YearBuilt', 'YearRemodAdd']\n",
    "\n",
    "cols = [*cols_categorical, *cols_continuous]\n",
    "assert len(cols) == len(cols_categorical) + len(cols_continuous)\n",
    "assert len(train.columns) - 1 == len(cols_to_drop) + len(cols) + len(cols_to_band)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e388460",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-02T07:48:07.405042Z",
     "iopub.status.busy": "2021-10-02T07:48:07.404286Z",
     "iopub.status.idle": "2021-10-02T07:48:07.662539Z",
     "shell.execute_reply": "2021-10-02T07:48:07.662017Z",
     "shell.execute_reply.started": "2021-10-02T07:45:06.010016Z"
    },
    "papermill": {
     "duration": 0.279346,
     "end_time": "2021-10-02T07:48:07.662697",
     "exception": false,
     "start_time": "2021-10-02T07:48:07.383351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# Some categorical features have null value as a category\n",
    "# This will cause errors during preprocessing: replace it with a string\n",
    "for data in [train, test]:\n",
    "    data[\"Alley\"].fillna('NA', inplace=True)\n",
    "    data[\"MasVnrType\"].fillna('NA', inplace=True)\n",
    "    data[\"BsmtQual\"].fillna('NA', inplace=True)\n",
    "    data[\"BsmtCond\"].fillna('NA', inplace=True)\n",
    "    data[\"BsmtExposure\"].fillna('NA', inplace=True)\n",
    "    data[\"BsmtFinType1\"].fillna('NA', inplace=True)\n",
    "    data[\"BsmtFinType2\"].fillna('NA', inplace=True)\n",
    "    data[\"FireplaceQu\"].fillna('NA', inplace=True)\n",
    "    data[\"GarageType\"].fillna('NA', inplace=True)\n",
    "    data[\"GarageFinish\"].fillna('NA', inplace=True)\n",
    "    data[\"GarageQual\"].fillna('NA', inplace=True)\n",
    "    data[\"GarageCond\"].fillna('NA', inplace=True)\n",
    "    data[\"PoolQC\"].fillna('NA', inplace=True)\n",
    "    data[\"Fence\"].fillna('NA', inplace=True)\n",
    "    data[\"MiscFeature\"].fillna('NA', inplace=True)\n",
    "\n",
    "# Some features in train/test dataset are missing\n",
    "# For categorical, fill with the mode; for continuous, fill with the mean\n",
    "for data in [train, test]:\n",
    "    for col in cols_categorical:\n",
    "        data[col].fillna(data[col].mode().values[0], inplace=True)\n",
    "    for col in cols_continuous:\n",
    "        data[col].fillna(data[col].mean(), inplace=True)\n",
    "    \n",
    "# Make categorical features numerical and ordinal by mean price\n",
    "# Normalize continuous features at the range of [0, 1]\n",
    "# Band features with sparse categories\n",
    "for col in cols_categorical:\n",
    "    process_categorical(train, col, test=test, show_stats=False)\n",
    "for col in cols_continuous:\n",
    "    process_continuous(train, col, test=test, show_stats=False)\n",
    "for col in cols_to_band:\n",
    "    band_categorical(train, col, test=test)\n",
    "\n",
    "# Some features can be combined for better inference\n",
    "for data in [train, test]:\n",
    "    data[\"Bath\"] = data[\"FullBath\"] + data[\"HalfBath\"]\n",
    "    data['YearBand'] = data['YearBuiltBand'] + data['YearRemodAddBand']\n",
    "for colname in ['FullBath', 'HalfBath']:\n",
    "    cols_categorical.remove(colname)\n",
    "    cols.remove(colname)\n",
    "for colname in ['Bath', 'YearBand']:\n",
    "    cols_categorical.insert(0, colname)\n",
    "    cols.insert(0, colname)\n",
    "    \n",
    "# The label feature is too large and too focused around the mean\n",
    "# Let's normalize it to make training more smooth\n",
    "# Since we need to de-normalize later, save the parameters\n",
    "train[\"SalePrice\"] = np.log(train[\"SalePrice\"])\n",
    "price_min = train[\"SalePrice\"].min()\n",
    "price_max = train[\"SalePrice\"].max()\n",
    "train[\"SalePrice\"] = (train[\"SalePrice\"] - price_min) / (price_max - price_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9667ab5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-02T07:48:07.690760Z",
     "iopub.status.busy": "2021-10-02T07:48:07.689692Z",
     "iopub.status.idle": "2021-10-02T07:48:08.695658Z",
     "shell.execute_reply": "2021-10-02T07:48:08.696331Z",
     "shell.execute_reply.started": "2021-10-02T07:45:06.292305Z"
    },
    "papermill": {
     "duration": 1.025488,
     "end_time": "2021-10-02T07:48:08.696558",
     "exception": false,
     "start_time": "2021-10-02T07:48:07.671070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Accuracy: 0.8723\n",
      "SGD Regressor Accuracy: -2.7431339436310936e+16\n",
      "Support Vector Machine Accuracy: 0.8503\n",
      "Kernel Ridge Accuracy: 0.8673\n",
      "Elastic Net Accuracy: 0.1882\n",
      "Bayesian Ridge Accuracy: 0.8715\n",
      "Gradient Boosting Regressor Accuracy: 0.9507\n"
     ]
    }
   ],
   "source": [
    "# Train various sklearn models\n",
    "sk_models = {\n",
    "    \"Linear Regression\": {\"model\": sklearn.linear_model.LinearRegression(), \"accuracy\": None},\n",
    "    \"SGD Regressor\": {\"model\": sklearn.linear_model.SGDRegressor(), \"accuracy\": None},\n",
    "    \"Support Vector Machine\": {\"model\": sklearn.svm.SVR(), \"accuracy\": None},\n",
    "    \"Kernel Ridge\": {\"model\": sklearn.kernel_ridge.KernelRidge(), \"accuracy\": None},\n",
    "    \"Elastic Net\": {\"model\": sklearn.linear_model.ElasticNet(), \"accuracy\": None},\n",
    "    \"Bayesian Ridge\": {\"model\": sklearn.linear_model.BayesianRidge(), \"accuracy\": None},\n",
    "    \"Gradient Boosting Regressor\": {\"model\": sklearn.ensemble.GradientBoostingRegressor(), \"accuracy\": None},\n",
    "}\n",
    "\n",
    "for sk_model_name in sk_models:\n",
    "    sk_model = sk_models[sk_model_name][\"model\"]\n",
    "    sk_model.fit(train[cols], train['SalePrice'])\n",
    "    sk_model_acc = round(sk_model.score(train[cols], train['SalePrice']), 4)\n",
    "    sk_models[sk_model_name][\"accuracy\"] = sk_model_acc\n",
    "    print(\"{} Accuracy: {}\".format(sk_model_name, sk_model_acc))\n",
    "\n",
    "prediction_sk = sk_models['Gradient Boosting Regressor'][\"model\"].predict(test[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da532249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-02T07:48:08.719103Z",
     "iopub.status.busy": "2021-10-02T07:48:08.718432Z",
     "iopub.status.idle": "2021-10-02T07:48:09.175465Z",
     "shell.execute_reply": "2021-10-02T07:48:09.174974Z",
     "shell.execute_reply.started": "2021-10-02T07:47:03.392457Z"
    },
    "papermill": {
     "duration": 0.468768,
     "end_time": "2021-10-02T07:48:09.175604",
     "exception": false,
     "start_time": "2021-10-02T07:48:08.706836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-02 07:48:08.911921: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-02 07:48:08.914899: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n",
      "2021-10-02 07:48:08.914941: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-10-02 07:48:08.914967: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (35bdf45af6aa): /proc/driver/nvidia/version does not exist\n",
      "2021-10-02 07:48:08.915286: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-02 07:48:08.915587: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "# Build deep learning model\n",
    "embeddings_dim = 32\n",
    "hidden_dim = 256\n",
    "\n",
    "def build_model():\n",
    "    # 1. Continuous feature inputs\n",
    "    continuous_inputs = tf.keras.Input(shape=(len(cols_continuous),))\n",
    "    _ = tf.keras.layers.Dense(units=hidden_dim, \n",
    "                              activation='relu')(continuous_inputs)\n",
    "    _ = tf.keras.layers.Dense(units=embeddings_dim, \n",
    "                              activation='relu')(_)\n",
    "    _ = tf.keras.layers.Reshape(target_shape=(1, embeddings_dim))(_)\n",
    "    \n",
    "    # 2. Discrete feature inputs\n",
    "    discrete_inputs = []\n",
    "    embeddings = []\n",
    "    for idx, col in enumerate(cols_categorical):\n",
    "        discrete_inputs.append(tf.keras.Input(shape=(1,)))\n",
    "        embeddings.append(\n",
    "            tf.keras.layers.Embedding(input_dim=train[col].max() + 1, \n",
    "                                      output_dim=embeddings_dim, \n",
    "                                      input_length=1,\n",
    "                                      embeddings_regularizer='l2')(discrete_inputs[idx]))\n",
    "        \n",
    "    # 3. Concatenation & Final MLP\n",
    "    _ = tf.keras.layers.Concatenate()([_, *embeddings])\n",
    "    _ = tf.keras.layers.Flatten()(_)\n",
    "    _ = tf.keras.layers.Dense(units=hidden_dim, \n",
    "                              activation='relu', \n",
    "                              kernel_regularizer='l2')(_)\n",
    "    _ = tf.keras.layers.Dense(units=1, \n",
    "                              activation='sigmoid')(_)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[continuous_inputs, *discrete_inputs], \n",
    "                       outputs=_, name=\"house_price_model\")\n",
    "\n",
    "model = build_model()\n",
    "# model.summary()\n",
    "# tf.keras.utils.plot_model(\n",
    "#     model,\n",
    "#     show_shapes=True,\n",
    "#     show_dtype=True,\n",
    "#     show_layer_names=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c39b83e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-02T07:48:09.200621Z",
     "iopub.status.busy": "2021-10-02T07:48:09.197792Z",
     "iopub.status.idle": "2021-10-02T07:48:18.726918Z",
     "shell.execute_reply": "2021-10-02T07:48:18.726326Z",
     "shell.execute_reply.started": "2021-10-02T07:47:09.906636Z"
    },
    "papermill": {
     "duration": 9.541833,
     "end_time": "2021-10-02T07:48:18.727088",
     "exception": false,
     "start_time": "2021-10-02T07:48:09.185255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-02 07:48:09.348903: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-10-02 07:48:09.361455: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2200150000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "19/19 [==============================] - 4s 53ms/step - loss: 2.8218 - mean_squared_error: 0.0139 - val_loss: 1.1827 - val_mean_squared_error: 0.0079\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 0.9399 - mean_squared_error: 0.0077 - val_loss: 0.3907 - val_mean_squared_error: 0.0079\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 0.2915 - mean_squared_error: 0.0059 - val_loss: 0.0886 - val_mean_squared_error: 0.0062\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 0.0607 - mean_squared_error: 0.0059 - val_loss: 0.0137 - val_mean_squared_error: 0.0059\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 0.0110 - mean_squared_error: 0.0061 - val_loss: 0.0075 - val_mean_squared_error: 0.0058\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 0.0067 - mean_squared_error: 0.0051 - val_loss: 0.0071 - val_mean_squared_error: 0.0055\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 0.0064 - mean_squared_error: 0.0049 - val_loss: 0.0102 - val_mean_squared_error: 0.0088\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 0.0074 - mean_squared_error: 0.0059 - val_loss: 0.0068 - val_mean_squared_error: 0.0053\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 0.0063 - mean_squared_error: 0.0049 - val_loss: 0.0080 - val_mean_squared_error: 0.0069\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 12ms/step - loss: 0.0076 - mean_squared_error: 0.0062 - val_loss: 0.0095 - val_mean_squared_error: 0.0080\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.0070 - mean_squared_error: 0.0057 - val_loss: 0.0063 - val_mean_squared_error: 0.0051\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 0.0066 - mean_squared_error: 0.0054 - val_loss: 0.0063 - val_mean_squared_error: 0.0051\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 0.0057 - mean_squared_error: 0.0045 - val_loss: 0.0065 - val_mean_squared_error: 0.0054\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.0060 - mean_squared_error: 0.0048 - val_loss: 0.0064 - val_mean_squared_error: 0.0052\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.0067 - mean_squared_error: 0.0055 - val_loss: 0.0069 - val_mean_squared_error: 0.0056\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 0.0070 - mean_squared_error: 0.0057 - val_loss: 0.0063 - val_mean_squared_error: 0.0051\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 0.0056 - mean_squared_error: 0.0045 - val_loss: 0.0061 - val_mean_squared_error: 0.0049\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 0.0057 - mean_squared_error: 0.0046 - val_loss: 0.0062 - val_mean_squared_error: 0.0049\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 0.0063 - mean_squared_error: 0.0052 - val_loss: 0.0065 - val_mean_squared_error: 0.0050\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.0070 - mean_squared_error: 0.0058 - val_loss: 0.0059 - val_mean_squared_error: 0.0048\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.0060 - mean_squared_error: 0.0048 - val_loss: 0.0062 - val_mean_squared_error: 0.0050\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 0.0060 - mean_squared_error: 0.0048 - val_loss: 0.0060 - val_mean_squared_error: 0.0049\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.0055 - mean_squared_error: 0.0044 - val_loss: 0.0059 - val_mean_squared_error: 0.0049\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 0.0055 - mean_squared_error: 0.0044 - val_loss: 0.0064 - val_mean_squared_error: 0.0052\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 0.0055 - mean_squared_error: 0.0044 - val_loss: 0.0092 - val_mean_squared_error: 0.0078\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.0069 - mean_squared_error: 0.0057 - val_loss: 0.0071 - val_mean_squared_error: 0.0057\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.0059 - mean_squared_error: 0.0047 - val_loss: 0.0058 - val_mean_squared_error: 0.0048\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.0049 - mean_squared_error: 0.0038 - val_loss: 0.0068 - val_mean_squared_error: 0.0055\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.0056 - mean_squared_error: 0.0046 - val_loss: 0.0086 - val_mean_squared_error: 0.0069\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 0.0058 - mean_squared_error: 0.0046 - val_loss: 0.0066 - val_mean_squared_error: 0.0053\n",
      "R^2: 0.7100372944030391\n"
     ]
    }
   ],
   "source": [
    "# Train deep learning model\n",
    "X_train, X_val, Y_train, Y_val = sklearn.model_selection.train_test_split(\n",
    "    train[cols].values, train[\"SalePrice\"].values, test_size=0.2, random_state=0)\n",
    "X_train = [X_train[:, -len(cols_continuous):], *[X_train[:, i] for i in range(len(cols_categorical))]]\n",
    "X_val = [X_val[:, -len(cols_continuous):], *[X_val[:, i] for i in range(len(cols_categorical))]]\n",
    "X_test = [test[cols].values[:, -len(cols_continuous):], *[test[cols].values[:, i] for i in range(len(cols_categorical))]]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=['mean_squared_error'],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    batch_size=64,\n",
    "    epochs=30,\n",
    "    verbose=1,\n",
    "    validation_data=(X_val, Y_val),\n",
    ")\n",
    "\n",
    "val_pred = model(X_val).numpy().reshape((-1,))\n",
    "score = np.corrcoef(val_pred, Y_val)[0, 1] ** 2\n",
    "print('R^2:', score)\n",
    "prediction_tf = model(X_test).numpy().reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4460c383",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-02T07:48:18.794956Z",
     "iopub.status.busy": "2021-10-02T07:48:18.794014Z",
     "iopub.status.idle": "2021-10-02T07:48:18.806111Z",
     "shell.execute_reply": "2021-10-02T07:48:18.806574Z",
     "shell.execute_reply.started": "2021-09-30T13:23:23.194312Z"
    },
    "papermill": {
     "duration": 0.047568,
     "end_time": "2021-10-02T07:48:18.806766",
     "exception": false,
     "start_time": "2021-10-02T07:48:18.759198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# De-normalize predictions and generate submission\n",
    "prediction = prediction_sk\n",
    "prediction = np.exp(prediction * (price_max - price_min) + price_min)\n",
    "submission = pd.DataFrame({'Id': test['Id'], 'SalePrice': prediction})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28.820142,
   "end_time": "2021-10-02T07:48:22.129019",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-10-02T07:47:53.308877",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
